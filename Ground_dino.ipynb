{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b84979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, SamProcessor, SamModel # Added SamProcessor, SamModel\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import torchmetrics\n",
    "\n",
    "# Define device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def imsc(img, *args, quiet=False, lim=None, interpolation='lanczos', **kwargs):\n",
    "    \"\"\"\n",
    "    Rescale and display an image represented as a tensor or PIL Image.\n",
    "    The function scales the img to the [0, 1] range.\n",
    "    The img is assumed to have shape 3xHxW (RGB) or 1xHxW (grayscale).\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor or PIL.Image): image.\n",
    "        quiet (bool, optional): if False, display image. Default: False.\n",
    "        lim (list, optional): [min, max] for rescaling. Default: None.\n",
    "        interpolation (str, optional): Interpolation mode for imshow. Default: 'lanczos'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Rescaled image as numpy array.\n",
    "    \"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = pil_to_tensor(img).float()\n",
    "    handle = None\n",
    "    with torch.no_grad():\n",
    "        if lim is None:\n",
    "            lim = [img.min(), img.max()]\n",
    "        img = img - lim[0]  # also makes a copy\n",
    "        img.mul_(1 / (lim[1] - lim[0]))\n",
    "        img = torch.clamp(img, min=0, max=1)\n",
    "        if not quiet:\n",
    "            # Ensure 3 channels for display\n",
    "            if img.shape[0] == 1:\n",
    "                img = img.expand(3, *img.shape[1:])\n",
    "            bitmap = img.permute(1, 2, 0).cpu().numpy()\n",
    "            return bitmap\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_images = torch.load('data/example_images_coco.pth', map_location=device)\n",
    "vaihingen_images = torch.load('data/example_images_vaihingen.pth', map_location=device)\n",
    "coco_labels = torch.load('data/example_labels_coco.pth', map_location=device)\n",
    "vaihingen_labels = torch.load('data/example_labels_vaihingen.pth', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43799f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mammo_images = torch.load('data/mammo_birads5.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d386aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_categories = \"bicycle.car.motorcycle.airplane.bus.train.truck.boat.traffic light.fire hydrant.stop sign.parking meter.bench.bird.cat.dog.horse.sheep.cow.elephant.bear.zebra.giraffe.backpack.umbrella.handbag.tie.suitcase.frisbee.skis.snowboard.sports ball.kite.baseball bat.baseball glove.skateboard.surfboard.tennis racket.bottle.wine glass.cup.fork.knife.spoon.bowl.banana.apple.sandwich.orange.broccoli.carrot.hot dog.pizza.donut.cake.chair.couch.potted plant.bed.dining table.toilet.tv.laptop.mouse.remote.keyboard.cell phone.microwave.oven.toaster.sink.refrigerator.book.clock.vase.scissors.teddy bear.hair drier.toothbrush\"\n",
    "vaihingen_categories = \"Impervious surfaces.Buildings.Low vegetation.Trees.Cars.Clutter\"\n",
    "print(f\"Created coco_categories string with {len(coco_categories.split('.'))} categories.\")\n",
    "print(f\"Created vaihingen_categories string with {len(vaihingen_categories.split('.'))} categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_iou_torchmetrics(\n",
    "    predicted_masks: torch.Tensor, \n",
    "    ground_truth_masks: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the mean Intersection over Union (IoU) for a batch of binary \n",
    "    segmentation masks using the torchmetrics library.\n",
    "\n",
    "    Args:\n",
    "        predicted_masks (torch.Tensor): A batch of predicted binary segmentation masks.\n",
    "                                        Expected shape: (N, H, W), where N is the batch size,\n",
    "                                        H is height, and W is width.\n",
    "                                        Values can be boolean, integer (0 or 1), or float (probabilities).\n",
    "        ground_truth_masks (torch.Tensor): A batch of ground truth binary segmentation masks.\n",
    "                                           Expected shape: (N, H, W).\n",
    "                                           Values can be boolean, integer (0 or 1), or float.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor representing the mean IoU for the batch.\n",
    "    \"\"\"\n",
    "    if not isinstance(predicted_masks, torch.Tensor) or not isinstance(ground_truth_masks, torch.Tensor):\n",
    "        raise TypeError(\"Inputs must be PyTorch Tensors.\")\n",
    "\n",
    "    if predicted_masks.shape != ground_truth_masks.shape:\n",
    "        raise ValueError(\n",
    "            f\"Shape mismatch: predicted_masks have shape {predicted_masks.shape} \"\n",
    "            f\"while ground_truth_masks have shape {ground_truth_masks.shape}.\"\n",
    "        )\n",
    "    if predicted_masks.ndim != 3:\n",
    "        raise ValueError(\n",
    "            f\"Input tensors must be 3-dimensional (N, H, W). \"\n",
    "            f\"Got {predicted_masks.ndim} dimensions.\"\n",
    "        )\n",
    "\n",
    "    # Ensure masks are on the same device\n",
    "    if predicted_masks.device != ground_truth_masks.device:\n",
    "        try:\n",
    "            ground_truth_masks = ground_truth_masks.to(predicted_masks.device)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not move ground_truth_masks to device {predicted_masks.device}. \"\n",
    "                f\"Ensure both tensors are on the same device. Error: {e}\"\n",
    "            )\n",
    "    \n",
    "    metric_device = predicted_masks.device\n",
    "\n",
    "    # Convert predicted masks to long integers (0 or 1)\n",
    "    if predicted_masks.dtype == torch.bool:\n",
    "        preds = predicted_masks.long()\n",
    "    elif predicted_masks.dtype.is_floating_point:\n",
    "        preds = (predicted_masks > 0.5).long() # Threshold probabilities\n",
    "    elif predicted_masks.dtype in [torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8]:\n",
    "        preds = predicted_masks.long()\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"predicted_masks dtype {predicted_masks.dtype} not supported. \"\n",
    "            \"Expected bool, float, or int.\"\n",
    "        )\n",
    "\n",
    "    # Convert ground truth masks to long integers (0 or 1)\n",
    "    if ground_truth_masks.dtype == torch.bool:\n",
    "        target = ground_truth_masks.long()\n",
    "    elif ground_truth_masks.dtype.is_floating_point:\n",
    "        # Assuming ground truth floats are already 0.0 or 1.0, or need thresholding\n",
    "        target = (ground_truth_masks > 0.5).long() \n",
    "    elif ground_truth_masks.dtype in [torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8]:\n",
    "        target = ground_truth_masks.long()\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"ground_truth_masks dtype {ground_truth_masks.dtype} not supported. \"\n",
    "            \"Expected bool, float, or int.\"\n",
    "        )\n",
    "\n",
    "    # Initialize Jaccard Index for binary task\n",
    "    # JaccardIndex is equivalent to IoU\n",
    "    jaccard = torchmetrics.JaccardIndex(task=\"binary\").to(metric_device)\n",
    "\n",
    "    # Compute IoU for the batch (mean IoU)\n",
    "    iou_score = jaccard(preds, target)\n",
    "\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04729bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_masks_list = []\n",
    "all_ground_truth_masks_list = []\n",
    "\n",
    "print(f\"Starting IoU evaluation for {len(coco_images)} COCO images...\")\n",
    "\n",
    "# Store the original matplotlib backend and turn off interactive plotting\n",
    "import matplotlib\n",
    "original_matplotlib_backend = None\n",
    "try:\n",
    "    original_matplotlib_backend = matplotlib.get_backend()\n",
    "except Exception: # Handle cases where backend might not be set or causes error\n",
    "    pass\n",
    "plt.ioff() # Turn off interactive mode to prevent plots from showing\n",
    "\n",
    "# Backup plotting functions to suppress them during the loop\n",
    "fig_axs_backup = plt.subplots\n",
    "plt_show_backup = plt.show\n",
    "\n",
    "def no_plot_subplots_factory(original_subplots_func):\n",
    "    def no_plot_subplots_impl(*args, **kwargs):\n",
    "        fig, axs = original_subplots_func(*args, **kwargs)\n",
    "        plt.close(fig) # Close the figure immediately\n",
    "        return fig, axs\n",
    "    return no_plot_subplots_impl\n",
    "\n",
    "def no_show_factory():\n",
    "    return lambda: None\n",
    "\n",
    "\n",
    "for i in range(len(coco_images)):\n",
    "    print(f\"Processing image {i+1}/{len(coco_images)}...\")\n",
    "    \n",
    "    if coco_labels is None or i >= len(coco_labels) or coco_labels[i] is None:\n",
    "        print(f\"Skipping image {i} due to missing ground truth label.\")\n",
    "        continue\n",
    "\n",
    "    # Suppress plots for this iteration\n",
    "    plt.subplots = no_plot_subplots_factory(fig_axs_backup)\n",
    "    plt.show = no_show_factory()\n",
    "\n",
    "    segmentation_output = segment_objects_on_image(\n",
    "        images_dataset=coco_images,\n",
    "        image_index=i,\n",
    "        text_prompts_str=coco_categories, \n",
    "        ground_truth_labels=coco_labels,\n",
    "        verbose=False \n",
    "    )\n",
    "    \n",
    "    # Restore plotting functions immediately after the call\n",
    "    plt.subplots = fig_axs_backup\n",
    "    plt.show = plt_show_backup\n",
    "\n",
    "    if segmentation_output is None or \\\n",
    "       segmentation_output.get('sam_masks') is None or \\\n",
    "       segmentation_output.get('sam_results') is None or \\\n",
    "       segmentation_output['sam_results'].iou_scores is None:\n",
    "        print(f\"Skipping image {i} due to missing segmentation results or IoU scores.\")\n",
    "        continue\n",
    "\n",
    "    predicted_sam_masks = segmentation_output['sam_masks'] \n",
    "    sam_iou_scores = segmentation_output['sam_results'].iou_scores\n",
    "\n",
    "    current_gt_mask_tensor = coco_labels[i]\n",
    "    # Assuming GT tensor is (C, H, W) or (H, W). We need H, W.\n",
    "    if current_gt_mask_tensor.ndim == 3:\n",
    "        h_gt, w_gt = current_gt_mask_tensor.shape[1], current_gt_mask_tensor.shape[2]\n",
    "    elif current_gt_mask_tensor.ndim == 2:\n",
    "        h_gt, w_gt = current_gt_mask_tensor.shape[0], current_gt_mask_tensor.shape[1]\n",
    "    else:\n",
    "        print(f\"Skipping image {i} due to unexpected ground truth mask dimensions: {current_gt_mask_tensor.ndim}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    if predicted_sam_masks is None or len(predicted_sam_masks) == 0:\n",
    "        print(f\"No objects detected/segmented for image {i}. Creating an empty predicted mask.\")\n",
    "        # Create an empty mask with the same H, W as the ground truth\n",
    "        joined_predicted_mask_np = np.zeros((h_gt, w_gt), dtype=bool)\n",
    "    else:\n",
    "        # Initialize a single mask for the current image (all objects combined)\n",
    "        joined_predicted_mask_np = np.zeros((h_gt, w_gt), dtype=bool)\n",
    "        \n",
    "        for obj_idx, single_object_masks_tensor in enumerate(predicted_sam_masks):\n",
    "            # single_object_masks_tensor has shape (3, H_pred, W_pred)\n",
    "            # sam_iou_scores[0, obj_idx, :] has shape (3,)\n",
    "            if obj_idx >= sam_iou_scores.shape[1]:\n",
    "                print(f\"Warning: obj_idx {obj_idx} out of bounds for sam_iou_scores second dimension {sam_iou_scores.shape[1]} on image {i}. Skipping object.\")\n",
    "                continue\n",
    "\n",
    "            best_mask_idx = torch.argmax(sam_iou_scores[0, obj_idx, :])\n",
    "            best_mask_for_object_tensor = single_object_masks_tensor[best_mask_idx] # Shape (H_pred, W_pred)\n",
    "            \n",
    "            best_mask_for_object_np = best_mask_for_object_tensor.cpu().numpy().astype(bool)\n",
    "            \n",
    "            # Ensure predicted mask is resized to GT dimensions if different\n",
    "            if best_mask_for_object_np.shape != joined_predicted_mask_np.shape:\n",
    "                # print(f\"Resizing predicted mask for object {obj_idx} from {best_mask_for_object_np.shape} to {joined_predicted_mask_np.shape}\")\n",
    "                pil_img = Image.fromarray(best_mask_for_object_np)\n",
    "                # PIL resize expects (width, height)\n",
    "                pil_img_resized = pil_img.resize((w_gt, h_gt), Image.NEAREST)\n",
    "                best_mask_for_object_np = np.array(pil_img_resized)\n",
    "\n",
    "            joined_predicted_mask_np |= best_mask_for_object_np\n",
    "\n",
    "    # Prepare ground truth mask (binarize)\n",
    "    gt_mask_np = current_gt_mask_tensor.cpu().numpy()\n",
    "    # Handle different GT mask shapes (e.g., (C,H,W) or (H,W))\n",
    "    if gt_mask_np.ndim == 3:\n",
    "        if gt_mask_np.shape[0] == 1: # Single channel (1,H,W)\n",
    "            gt_mask_np = gt_mask_np.squeeze(0)\n",
    "        else: # Multi-channel (C,H,W), e.g. for semantic segmentation with class IDs\n",
    "              # For binary IoU against \"any object\", we can take a max projection or sum then binarize\n",
    "            # print(f\"Warning: GT mask for image {i} has shape {gt_mask_np.shape}. Taking max across channels for binarization.\")\n",
    "            gt_mask_np = np.max(gt_mask_np, axis=0) \n",
    "            \n",
    "    gt_mask_bin_np = (gt_mask_np > 0) # Binarize: True for any labeled pixel\n",
    "\n",
    "    # Final check for shape consistency before appending\n",
    "    if joined_predicted_mask_np.shape != gt_mask_bin_np.shape:\n",
    "        print(f\"Error: Final shape mismatch for image {i}. Pred: {joined_predicted_mask_np.shape}, GT: {gt_mask_bin_np.shape}. Skipping this image.\")\n",
    "        continue\n",
    "\n",
    "    all_predicted_masks_list.append(torch.from_numpy(joined_predicted_mask_np)) \n",
    "    all_ground_truth_masks_list.append(torch.from_numpy(gt_mask_bin_np))\n",
    "\n",
    "# Restore matplotlib backend and turn interactive mode back on\n",
    "if original_matplotlib_backend:\n",
    "    try:\n",
    "        matplotlib.use(original_matplotlib_backend)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not restore matplotlib backend: {e}\")\n",
    "plt.ion() # Turn interactive mode back on\n",
    "\n",
    "if all_predicted_masks_list and all_ground_truth_masks_list:\n",
    "    # Stack all masks into batch tensors\n",
    "    try:\n",
    "        # Ensure all tensors in the list have the same shape before stacking\n",
    "        # This should be handled by the resizing logic above, but good to be aware\n",
    "        batched_preds = torch.stack(all_predicted_masks_list).to(device) # Shape: (N, H, W)\n",
    "        batched_gts = torch.stack(all_ground_truth_masks_list).to(device)   # Shape: (N, H, W)\n",
    "\n",
    "        # Calculate IoU using the torchmetrics function\n",
    "        mean_iou = calculate_batch_iou_torchmetrics(batched_preds, batched_gts)\n",
    "        print(f\"\\nMean IoU (torchmetrics) for the COCO dataset batch: {mean_iou.item():.4f}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during stacking masks or calculating IoU: {e}\")\n",
    "        print(\"This might be due to inconsistent mask shapes across the batch. Check processing steps and warnings.\")\n",
    "else:\n",
    "    print(\"No masks were collected to calculate batch IoU. Please check processing steps and data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_on_image(images,image_index, text_prompts_str, labels, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs zero-shot object detection on a specified image from coco_images\n",
    "    using a given text prompt. Displays the image with detections and prints results.\n",
    "\n",
    "    Args:\n",
    "        image_index (int): Index of the image in the global coco_images tensor.\n",
    "        text_prompts_str (str): Text prompt describing objects to detect (e.g., \"a cat . a dog .\").\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the detection results (scores, labels, boxes), or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Ensure global variables coco_images, processor, model are accessible.\n",
    "    # Ensure necessary imports: Image, ImageDraw, plt, torch, np are available from previous cells.\n",
    "\n",
    "    # 1. Select and prepare the image from coco_images\n",
    "    if not (0 <= image_index < len(images)):\n",
    "        print(f\"Error: image_index {image_index} is out of bounds for coco_images (size {len(images)}).\")\n",
    "        return None\n",
    "\n",
    "    image_tensor = images[image_index] # Expected shape: (C, H, W)\n",
    "\n",
    "    # Permute to HWC for PIL and Matplotlib if it's CHW\n",
    "    processed_image_tensor = image_tensor\n",
    "    if len(image_tensor.shape) == 3 and image_tensor.shape[0] == 3: # CHW\n",
    "        processed_image_tensor = image_tensor.permute(1, 2, 0) # HWC\n",
    "    # Add handling for other shapes if necessary, though coco_images[idx] should be (C,H,W)\n",
    "\n",
    "    image_np = processed_image_tensor.cpu().numpy()\n",
    "\n",
    "    # Normalize to 0-1 range based on its own min/max, then scale to 0-255 for PIL\n",
    "    min_val = image_np.min()\n",
    "    max_val = image_np.max()\n",
    "    if max_val == min_val: # Handle uniform images (e.g., all black or all white)\n",
    "        image_scaled_np = np.zeros_like(image_np, dtype=np.float32)\n",
    "    else:\n",
    "        image_scaled_np = (image_np - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Convert to PIL Image. Assumes image_scaled_np is HWC and in [0,1]\n",
    "    image_pil = Image.fromarray((image_scaled_np * 255).astype(np.uint8))\n",
    "\n",
    "    # 2. Prepare inputs for the model\n",
    "    inputs = processor(images=image_pil, text=text_prompts_str, return_tensors=\"pt\")\n",
    "    \n",
    "    # Optionally, move inputs to model.device if experiencing device mismatches\n",
    "    # inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 3. Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # 4. Post-process the outputs\n",
    "    target_sizes = [image_pil.size[::-1]]  # PIL size is (width, height), model expects (height, width)\n",
    "    \n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids, \n",
    "        box_threshold=0.4, \n",
    "        text_threshold=0.3, \n",
    "        target_sizes=target_sizes\n",
    "    )[0] # Get results for the first (and only) image processed\n",
    "\n",
    "    # 5. Draw bounding boxes\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"text_labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        draw.rectangle(box, outline=\"red\", width=1) # Using width=1 as per current notebook state\n",
    "        draw.text((box[0], box[1]-10), f\"{label}: {round(score.item(), 2)}\", fill=\"red\")\n",
    "\n",
    "    # 6. Display the image with detections and the ground truth label side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Left: Detected objects\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f\"Detections for prompt: '{text_prompts_str if len(text_prompts_str) < 50 else 'COCO labels'}'\\nimage index {image_index}\")\n",
    "\n",
    "    if labels is not None:\n",
    "        # Right: Ground truth label mask\n",
    "        gt_mask = labels[image_index].cpu().numpy()\n",
    "        axs[1].imshow(gt_mask, cmap='tab20')\n",
    "        axs[1].axis('off')\n",
    "        axs[1].set_title(\"COCO Ground Truth Label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if verbose:\n",
    "    # 7. Print detection details\n",
    "        print(f\"Detected objects: {results['labels']}\")\n",
    "        print(f\"Scores: {results['scores']}\")\n",
    "        print(f\"Boxes: {results['boxes']}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_objects_on_image(images_dataset, image_index, text_prompts_str, ground_truth_labels, verbose = False):\n",
    "    \"\"\"\n",
    "    Performs text-prompt based object detection and segmentation on a specified image.\n",
    "    Uses Grounding DINO for detection and SAM for segmentation.\n",
    "    Displays detections, segmentations, and ground truth.\n",
    "\n",
    "    Args:\n",
    "        images_dataset (torch.Tensor): The dataset of images (e.g., coco_images).\n",
    "        image_index (int): Index of the image in the images_dataset.\n",
    "        text_prompts_str (str): Text prompt describing objects to detect and segment.\n",
    "        ground_truth_labels (torch.Tensor): The ground truth segmentation masks for comparison.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing DINO detection results and SAM segmentation masks, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # Ensure global models (model, processor for DINO; sam_model, sam_processor for SAM) and device are accessible.\n",
    "    # Ensure necessary imports (Image, ImageDraw, plt, torch, np) are available.\n",
    "\n",
    "    # 1. Select and prepare the image\n",
    "    if not (0 <= image_index < len(images_dataset)):\n",
    "        print(f\"Error: image_index {image_index} is out of bounds for images_dataset (size {len(images_dataset)}).\")\n",
    "        return None\n",
    "        \n",
    "    image_tensor = images_dataset[image_index] \n",
    "\n",
    "    processed_image_tensor = image_tensor\n",
    "    if len(image_tensor.shape) == 3 and image_tensor.shape[0] == 3: # CHW to HWC for PIL\n",
    "        processed_image_tensor = image_tensor.permute(1, 2, 0)\n",
    "    \n",
    "    image_np = processed_image_tensor.cpu().numpy()\n",
    "\n",
    "    min_val, max_val = image_np.min(), image_np.max()\n",
    "    image_scaled_np = (image_np - min_val) / (max_val - min_val) if max_val > min_val else np.zeros_like(image_np)\n",
    "    \n",
    "    image_pil = Image.fromarray((image_scaled_np * 255).astype(np.uint8)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "    # 2. Object Detection with Grounding DINO\n",
    "    dino_inputs = processor(images=image_pil, text=text_prompts_str, return_tensors=\"pt\")\n",
    "    #dino_inputs = {k: v.to(device) for k, v in dino_inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dino_outputs = model(**dino_inputs)\n",
    "\n",
    "    dino_results_processed = processor.post_process_grounded_object_detection(\n",
    "        dino_outputs,\n",
    "        dino_inputs.input_ids, \n",
    "        box_threshold=0.4, \n",
    "        text_threshold=0.3, \n",
    "        target_sizes=[image_pil.size[::-1]] \n",
    "    )[0]\n",
    "\n",
    "    detected_boxes = dino_results_processed[\"boxes\"]\n",
    "    detected_labels = dino_results_processed[\"text_labels\"]\n",
    "    detected_scores = dino_results_processed[\"scores\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"DINO Detected objects: {detected_labels}\")\n",
    "        print(f\"DINO Scores: {detected_scores}\")\n",
    "        print(f\"DINO Boxes: {detected_boxes}\")\n",
    "\n",
    "    # 3. Image Segmentation with SAM (if objects were detected)\n",
    "    sam_masks_processed = None\n",
    "    if len(detected_boxes) > 0:\n",
    "        sam_inputs = sam_processor(image_pil, input_boxes=[detected_boxes.cpu().tolist()], return_tensors=\"pt\")\n",
    "        #sam_inputs = {k: v.to(device) for k, v in sam_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sam_outputs = sam_model(**sam_inputs)\n",
    "        \n",
    "        sam_masks_processed = sam_processor.image_processor.post_process_masks(\n",
    "            sam_outputs.pred_masks.cpu(), \n",
    "            sam_inputs[\"original_sizes\"].cpu(), \n",
    "            sam_inputs[\"reshaped_input_sizes\"].cpu()\n",
    "        )[0] # Get masks for the first (and only) image\n",
    "        if verbose:\n",
    "            print(f\"SAM generated {len(sam_masks_processed)} masks for {len(detected_boxes)} detected boxes.\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No objects detected by Grounding DINO, skipping SAM segmentation.\")\n",
    "\n",
    "    # 4. Visualization\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(36, 12))\n",
    "\n",
    "    # Plot 1: Detections (Image + DINO boxes + DINO labels)\n",
    "    image_dino_plot = image_pil.copy()\n",
    "    draw_dino = ImageDraw.Draw(image_dino_plot)\n",
    "    for score, label, box in zip(detected_scores, detected_labels, detected_boxes):\n",
    "        box_coords = [round(i, 2) for i in box.tolist()]\n",
    "        draw_dino.rectangle(box_coords, outline=\"red\", width=2)\n",
    "        draw_dino.text((box_coords[0], box_coords[1]-12), f\"{label}: {round(score.item(), 2)}\", fill=\"red\")\n",
    "    axs[0].imshow(image_dino_plot)\n",
    "    axs[0].set_title(f\"DINO Detections: '{text_prompts_str if len(text_prompts_str) < 50 else 'COCO labels'}'\\nimage index {image_index}\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Plot 2: Segmentations (Image + SAM masks)\n",
    "    axs[1].imshow(image_pil)\n",
    "    if sam_masks_processed is not None and len(sam_masks_processed) > 0:\n",
    "        #generate a list of bright rgb colors to be used for overlaying masks\n",
    "        colors = [\n",
    "            [255, 255, 102],  # Light Yellow\n",
    "            [173, 255, 47],   # Light Green (GreenYellow)\n",
    "            [135, 206, 250],  # Light Sky Blue\n",
    "            [255, 182, 193],  # Light Pink\n",
    "            [255, 160, 122],  # Light Salmon\n",
    "            [240, 255, 240],  # Honeydew (very light green)\n",
    "            [255, 250, 205],  # Lemon Chiffon\n",
    "            [224, 255, 255],  # Light Cyan\n",
    "            [250, 235, 215],  # Antique White\n",
    "            [255, 228, 181],  # Moccasin (Light Orange/Peach)\n",
    "            [245, 245, 220],  # Beige\n",
    "            [211, 211, 211],  # Light Grey\n",
    "            [175, 238, 238],  # Pale Turquoise\n",
    "            [255, 218, 185],  # Peach Puff\n",
    "            [255, 105, 180]   # Hot Pink (brighter, but still light)\n",
    "            ]\n",
    "        for i, mask_tensor in enumerate(sam_masks_processed): # Iterate over each object's mask set\n",
    "            # Assuming each item in sam_masks_processed is [num_masks_for_object, H, W]\n",
    "            # For Grounded-SAM, usually, one box leads to multiple masks, pick the best one (e.g., by area or use SAM scores if available)\n",
    "            # Here, we'll just show the first mask for each detected box for simplicity.\n",
    "            # The masks from SAM are boolean.\n",
    "            # mask_tensor shape is [H, W] after selecting one mask per box.\n",
    "            \n",
    "            # For multiple masks per box, you might need to select one.\n",
    "            # Here, let's assume sam_masks_processed is [num_detected_objects, H, W]\n",
    "            # Or if it's [1, num_detected_objects, H, W], squeeze it.\n",
    "            # The output of post_process_masks is typically [batch_size, num_objects, H, W]\n",
    "            # Since we have batch_size=1, it's [1, num_objects, H, W]. So sam_masks_processed[0] is [num_objects, H, W]\n",
    "            \n",
    "            # If sam_masks_processed is [N, H, W] where N is number of detected boxes\n",
    "            #print(mask_tensor.shape) # Debugging line to check mask shape\n",
    "            mask_to_show = mask_tensor.squeeze().cpu().numpy() # Take the mask for the i-th box\n",
    "            #provide a long list of bright colors for overlay\n",
    "            #colors = plt.cm.get_cmap('hsv', len(sam_masks_processed)) # Get a colormap with enough colors\n",
    "            color = colors[i%15]\n",
    "            overlay = np.zeros_like(image_scaled_np)\n",
    "            #permute to HWC for overlay, is an np.array\n",
    "            mask_to_show = np.moveaxis(mask_to_show, 0, -1) # Ensure mask_to_show is HWC\n",
    "            overlay[mask_to_show[:,:,np.argmax(sam_outputs.iou_scores[0,i,:])]] = color # Use a threshold, SAM masks are float [0,1]\n",
    "\n",
    "            axs[1].imshow(overlay.astype(np.uint8), alpha=0.3) # Overlay mask\n",
    "\n",
    "            # Add corresponding label text if possible\n",
    "            if i < len(detected_labels):\n",
    "                 box_coords = [round(coord, 2) for coord in detected_boxes[i].tolist()]\n",
    "                 axs[1].text(box_coords[0], box_coords[1]-12, detected_labels[i], color='white', backgroundcolor='black', fontsize=8)\n",
    "\n",
    "    axs[1].set_title(\"SAM Segmentations\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Plot 3: Ground Truth Label Mask\n",
    "    if ground_truth_labels is not None and 0 <= image_index < len(ground_truth_labels):\n",
    "        gt_mask = ground_truth_labels[image_index].cpu().numpy()\n",
    "        axs[2].imshow(gt_mask, cmap='tab20') # Assuming GT is a single mask with different class IDs\n",
    "        axs[2].set_title(\"Ground Truth Label\")\n",
    "    else:\n",
    "        axs[2].set_title(\"Ground Truth (Not available/plotted)\")\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"dino_results\": dino_results_processed, \"sam_masks\": sam_masks_processed, \"sam_results\": sam_outputs if sam_masks_processed is not None else None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    detect_objects_on_image(mammo_images, i, \"baloon.\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b782bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_objects_on_image(coco_images, 0, \"traffic light . car . person .\", coco_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd902598",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_objects_on_image(coco_images, 0, coco_categories, coco_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7de1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_objects_on_image(vaihingen_images, 0, \"cars . window . yellow car .\", vaihingen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results = segment_objects_on_image(\n",
    "    images_dataset=vaihingen_images, \n",
    "    image_index=0, \n",
    "    text_prompts_str=\" window . yellow car .\", \n",
    "    ground_truth_labels=vaihingen_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    segment_objects_on_image(\n",
    "        images_dataset=mammo_images, \n",
    "        image_index=i, \n",
    "        text_prompts_str=\"Jumbo jet.\", \n",
    "        ground_truth_labels=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = []\n",
    "for i in range(len(coco_images)):\n",
    "    print(f\"Segmenting image {i} of {len(coco_images)}\")\n",
    "    result = segment_objects_on_image(\n",
    "        images_dataset=coco_images, \n",
    "        image_index=i, \n",
    "        text_prompts_str=coco_categories, \n",
    "        ground_truth_labels=coco_labels\n",
    "    )\n",
    "    masks = result['sam_masks']\n",
    "    if masks is None or len(masks) == 0:\n",
    "        print(f\"No masks generated for image {i}. Skipping.\")\n",
    "        continue\n",
    "    iou_scores = result['sam_results'].iou_scores\n",
    "    iou = 0.0\n",
    "    joined_mask = np.zeros_like(coco_labels[i].cpu().numpy(), dtype=bool)\n",
    "    for j, mask in enumerate(masks):\n",
    "        mask = mask[np.argmax(iou_scores[0,j,:])]\n",
    "        \n",
    "        mask_np = mask.cpu().numpy() if hasattr(mask, 'cpu') else np.array(mask)\n",
    "        mask_np = mask_np.astype(bool)\n",
    "        # For binary IoU: treat all nonzero in gt_mask as foreground\n",
    "        joined_mask |= mask_np\n",
    "\n",
    "    gt_mask = coco_labels[i].cpu().numpy()\n",
    "    gt_mask_bin = gt_mask > 0\n",
    "\n",
    "    intersection = np.logical_and(joined_mask, gt_mask_bin).sum()\n",
    "    union = np.logical_or(joined_mask, gt_mask_bin).sum()\n",
    "    iou += intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    print(f\"IoU for image {i}: {iou:.4f}\")\n",
    "    ious.append(iou)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(ious)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
