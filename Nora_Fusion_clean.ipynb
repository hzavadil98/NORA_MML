{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNqGzYSAExl_"
      },
      "source": [
        "# MNIST Multi-Modal Learning Practice Notebook\n",
        "\n",
        "In this notebook, you will practice some of the core concepts we have presented. The overall pipeline is as follows:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Data Preparation\n",
        "\n",
        "- Load the **MNIST** dataset.\n",
        "- Split the dataset into **train**, **validation**, and **test** sets.\n",
        "- Horizontally split each image into **upper** and **lower** halves.\n",
        "- Pad the removed half with zeros to maintain consistent input shapes.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Model Definitions\n",
        "\n",
        "Define three encoders:\n",
        "\n",
        "1. **CNNEncoder**  \n",
        "   - A simple CNN with two convolutional layers and pooling layers.\n",
        "\n",
        "2. **MLPEncoder**  \n",
        "   - A simple MLP with two fully connected layers.\n",
        "\n",
        "3. **FusedModel**  \n",
        "   - A combined model using:\n",
        "     - `CNNEncoder` for the **upper half**.\n",
        "     - `MLPEncoder` for the **lower half**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training and Evaluation\n",
        "\n",
        "For each encoder:\n",
        "\n",
        "### (i) CNNEncoder\n",
        "\n",
        "- Use only the **upper half** of the input data.\n",
        "- Train for **5 epochs**.\n",
        "- Validate using the validation set.\n",
        "- Evaluate performance on the test set.\n",
        "\n",
        "### (ii) MLPEncoder\n",
        "\n",
        "- Use only the **lower half** of the input data.\n",
        "- Train for **5 epochs**.\n",
        "- Validate using the validation set.\n",
        "- Evaluate performance on the test set.\n",
        "\n",
        "### (iii) FusedModel\n",
        "\n",
        "- Use **both upper and lower halves** of the input data.\n",
        "- Use the CNNEncoder (upper half) and the MLPEncoder (bottom half) and use concatenation to fuse the representations.\n",
        "- Train for **5 epochs**.\n",
        "- Validate using the validation set.\n",
        "- Evaluate performance on the test set.\n",
        "\n",
        "### (iv) Different Fusion Stategies\n",
        "- Explore alternative fusion strategies such as for instance average fusion (Averaging representations).\n",
        "\n",
        "### (v) Investigate how adding noise to the representations impacts performance\n",
        "- Add increasing amounts of noise to one or both modalities and monitor performance.\n",
        "- Visualize the fused representations using PCA or t-SNE.\n",
        "\n",
        "---\n",
        "Concepts introduced tomorrow:\n",
        "\n",
        "### (vi) Self-supervised:\n",
        "- Instead of directly fusing modalities, align the modalities using CLIP.\n",
        "- Visualize the fused representations using PCA or t-SNE.\n",
        "- Train a linear classifier on-top of the learned representations (keeping the encoders frozen).\n",
        "\n",
        "### (vii) Alignment noise:\n",
        "- Add increasing amounts of noise to one or both modalities and monitor performance.\n",
        "- Visualize the fused representations using PCA or t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZSJS417JBMt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA2VUQ7pDmbu"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B3bQzJZLzL3T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qInXFkhcDnI_"
      },
      "source": [
        "# Set hyperparams and load, split MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ekrUg287zoED"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "num_epochs = 5\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "# Load MNIST dataset\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.7, 0.3])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN8RM6hTDx7m"
      },
      "source": [
        "# Visualize some images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FTQYoTcZ1HrZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    # Visualize the first few images in the batch\n",
        "    num_images_to_show = 5\n",
        "    for i in range(num_images_to_show):\n",
        "        image = images[i]  # Get one image from the batch\n",
        "        image = np.transpose(image, (1, 2, 0))  # Rearrange dimensions from CxHxW to HxWxC\n",
        "\n",
        "        # Display the image\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.title(f'Label: {labels[i].item()}')\n",
        "        plt.axis('off')  # Turn off axis labels\n",
        "        plt.show()\n",
        "\n",
        "    break  # Exit the loop after visualizing the first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJnnL37-D2ct"
      },
      "source": [
        "# Define our models - a CNN, an MLP, and a FusedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "flcfL5cAzrDC"
      },
      "outputs": [],
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    \"\"\"Simple CNN Encoder\"\"\"\n",
        "    def __init__(self):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64*7*7, 10) #This shape depends on the kernels and the input (split) shape\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.conv1(x[0]))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "    \n",
        "    def inner_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Extract inner features from the input tensor.\"\"\"\n",
        "        x = F.relu(self.conv1(x[0]))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Simple 2-Layer MLP\"\"\"\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.fc1(x[1].view(x[1].size(0), -1))) #Flatten all dimensions except batch_size\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "    \n",
        "    def inner_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Extract inner features from the input tensor.\"\"\"\n",
        "        x = F.relu(self.fc1(x[1].view(x[1].size(0), -1))) #Flatten all dimensions except batch_size\n",
        "        return x\n",
        "\n",
        "# Fusing representations\n",
        "class FusedModel(nn.Module):\n",
        "    #Implement this in practical\n",
        "    \"\"\"Model that fuses CNN and MLP representations.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(FusedModel, self).__init__()\n",
        "        self.cnn_encoder = CNNEncoder()\n",
        "        self.mlp_encoder = MLP()\n",
        "        self.fc = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        cnn_out = self.cnn_encoder(x)\n",
        "        mlp_out = self.mlp_encoder(x)\n",
        "        fused = torch.cat((cnn_out, mlp_out), dim=1)\n",
        "        return self.fc(fused)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYxUVujeECJs"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pUpLf1EvzuXZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def prepare_data(data):\n",
        "    \"\"\"Splits MNIST images into two halves horizontally and pads to original shape.\"\"\"\n",
        "    upper_half = data[:, :, :14, :]  # Top half: [B, 1, 14, 28]\n",
        "    lower_half = data[:, :, 14:, :]  # Bottom half: [B, 1, 14, 28]\n",
        "\n",
        "    # Pad bottom 14 rows with zeros for upper_half\n",
        "    upper_half_padded = F.pad(upper_half, pad=(0, 0, 0, 14))  # Pad rows: (left, right, top, bottom)\n",
        "\n",
        "    # Pad top 14 rows with zeros for lower_half\n",
        "    lower_half_padded = F.pad(lower_half, pad=(0, 0, 14, 0))  # Pad rows: (left, right, top, bottom)\n",
        "\n",
        "    return upper_half_padded, lower_half_padded\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, data_loader, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, device: torch.device):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for i, (images, labels) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        upper_half, lower_half = prepare_data(images)\n",
        "        upper_half = upper_half.to(device)\n",
        "        lower_half = lower_half.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model((upper_half, lower_half))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images[0].size(0)\n",
        "        #print(f\"Batch {i}: Loss = {loss.item()}\")\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "\n",
        "def evaluate_model(model: nn.Module, data_loader, criterion, device: torch.device):\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in data_loader:\n",
        "            upper_half, lower_half = prepare_data(images)\n",
        "            upper_half = upper_half.to(device)\n",
        "            lower_half = lower_half.to(device)\n",
        "            labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "            outputs = model((upper_half, lower_half))\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xN25tug51MFK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28]) torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "for images, _ in train_loader:\n",
        "  upper_half, lower_half = prepare_data(images)\n",
        "  print(upper_half.shape, lower_half.shape)\n",
        "  print(images.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKNhzJQNEHTF"
      },
      "source": [
        "# Check that split works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y_9UNU7315Qo"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Look at the first upper_half image\n",
        "image = upper_half[0]  # Get one image from the batch\n",
        "image = np.transpose(image, (1, 2, 0))  # Rearrange dimensions from CxHxW to HxWxC\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XABlOAXS2Lul"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Look at the first lower_half image\n",
        "image = lower_half[0]  # Get one image from the batch\n",
        "image = np.transpose(image, (1, 2, 0))  # Rearrange dimensions from CxHxW to HxWxC\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ciimlk8R2Wtr"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#look at first full image\n",
        "image = images[0]\n",
        "image = np.transpose(image, (1, 2, 0))  # Rearrange dimensions from CxHxW to HxWxC\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edplWa04ELkg"
      },
      "source": [
        "# Init and train CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "as_g7BSFz0PU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNNEncoder(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=3136, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Initialize CNN\n",
        "cnn_encoder = CNNEncoder()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "cnn_optimizer = optim.Adam(cnn_encoder.parameters(), lr=learning_rate)\n",
        "cnn_encoder.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xfp3nPJzz21t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: CNN Encoder train loss 0.007456954736439955\n",
            "Epoch 1: CNN Encoder val loss 0.31328244144717854, accuracy 0.9018\n",
            "Epoch 2: CNN Encoder train loss 0.0047920311668089455\n",
            "Epoch 2: CNN Encoder val loss 0.29500186036692727, accuracy 0.9057\n",
            "Epoch 3: CNN Encoder train loss 0.004613782350683496\n",
            "Epoch 3: CNN Encoder val loss 0.284083340883255, accuracy 0.9086\n",
            "Epoch 4: CNN Encoder train loss 0.004311020391150599\n",
            "Epoch 4: CNN Encoder val loss 0.2794670794539981, accuracy 0.9117\n",
            "Epoch 5: CNN Encoder train loss 0.00415596181225209\n",
            "Epoch 5: CNN Encoder val loss 0.29409962191846634, accuracy 0.9064\n",
            "Done training! Evaluating on test set...\n",
            "CNN Encoder test loss 0.2704895228385925, accuracy 0.9103\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the CNN encoder\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = train_model(cnn_encoder, train_loader, cnn_optimizer, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: CNN Encoder train loss {avg_loss}\")\n",
        "    avg_loss, accuracy = evaluate_model(cnn_encoder, val_loader, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: CNN Encoder val loss {avg_loss}, accuracy {accuracy:.4f}\")\n",
        "\n",
        "print(\"Done training! Evaluating on test set...\")\n",
        "#Test\n",
        "avg_loss, accuracy = evaluate_model(cnn_encoder, test_loader, criterion, device=device)\n",
        "print(f\"CNN Encoder test loss {avg_loss}, accuracy {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvNUOossEOKJ"
      },
      "source": [
        "# Init and train MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ut8P-NWF7U-n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: MLP train loss 0.02773515165135974\n",
            "Epoch 1: MLP val loss 1.7524439165327284, accuracy 0.2792\n",
            "Epoch 2: MLP train loss 0.026802735402470544\n",
            "Epoch 2: MLP val loss 1.7220430903964572, accuracy 0.2917\n",
            "Epoch 3: MLP train loss 0.026591129200799125\n",
            "Epoch 3: MLP val loss 1.7182743078867595, accuracy 0.2893\n",
            "Epoch 4: MLP train loss 0.026443898955980937\n",
            "Epoch 4: MLP val loss 1.6948569176991781, accuracy 0.2944\n",
            "Epoch 5: MLP train loss 0.026414682947454\n",
            "Epoch 5: MLP val loss 1.6918685295316909, accuracy 0.2927\n",
            "Done training! Evaluating on test set...\n",
            "MLP test loss 1.682537137413025, accuracy 0.2972\n"
          ]
        }
      ],
      "source": [
        "#Initialize MLP\n",
        "mlp = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "mlp_optimizer = optim.Adam(mlp.parameters(), lr=learning_rate)\n",
        "mlp.to(device)\n",
        "\n",
        "# Train and evaluate the MLP\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = train_model(mlp, train_loader, mlp_optimizer, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: MLP train loss {avg_loss}\")\n",
        "    avg_loss, accuracy = evaluate_model(mlp, val_loader, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: MLP val loss {avg_loss}, accuracy {accuracy:.4f}\")\n",
        "\n",
        "print(\"Done training! Evaluating on test set...\")\n",
        "#Test\n",
        "avg_loss, accuracy = evaluate_model(mlp, test_loader, criterion, device=device)\n",
        "print(f\"MLP test loss {avg_loss}, accuracy {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FWvHH5fEQch"
      },
      "source": [
        "# Init and train FusedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nAPrwScO7X-r"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FusedModel(\n",
              "  (cnn_encoder): CNNEncoder(\n",
              "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (fc1): Linear(in_features=3136, out_features=10, bias=True)\n",
              "  )\n",
              "  (mlp_encoder): MLP(\n",
              "    (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=20, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Initialize Fusion Encoder\n",
        "fused_nn = FusedModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "fuse_optimizer =  optim.Adam(fused_nn.parameters(), lr=learning_rate)\n",
        "fused_nn.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yxzCNfMB7m8y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Fusion Encoder train loss 0.005656766788856614\n",
            "Epoch 1: Fusion Encoder val loss 0.20919932531317076, accuracy 0.9400\n",
            "Epoch 2: Fusion Encoder train loss 0.0028697318988186974\n",
            "Epoch 2: Fusion Encoder val loss 0.16039971411890452, accuracy 0.9511\n",
            "Epoch 3: Fusion Encoder train loss 0.002510646524639534\n",
            "Epoch 3: Fusion Encoder val loss 0.14675224485662247, accuracy 0.9585\n",
            "Epoch 4: Fusion Encoder train loss 0.0022608802577347626\n",
            "Epoch 4: Fusion Encoder val loss 0.14241304373410013, accuracy 0.9584\n",
            "Epoch 5: Fusion Encoder train loss 0.0020517527315033865\n",
            "Epoch 5: Fusion Encoder val loss 0.1471181207464801, accuracy 0.9571\n",
            "Done training! Evaluating on test set...\n",
            "Fusion Encoder test loss 0.13777706570625306, accuracy 0.9615\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the Fused encoder\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = train_model(fused_nn, train_loader, fuse_optimizer, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: Fusion Encoder train loss {avg_loss}\")\n",
        "    avg_loss, accuracy = evaluate_model(fused_nn, val_loader, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: Fusion Encoder val loss {avg_loss}, accuracy {accuracy:.4f}\")\n",
        "\n",
        "print(\"Done training! Evaluating on test set...\")\n",
        "#Test\n",
        "avg_loss, accuracy = evaluate_model(fused_nn, test_loader, criterion, device=device)\n",
        "print(f\"Fusion Encoder test loss {avg_loss}, accuracy {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDTcxnS2KEDv"
      },
      "source": [
        "## (iv) Different Fusion Stategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jiDpuDoBJimr"
      },
      "outputs": [],
      "source": [
        "class FusedModel1(nn.Module):\n",
        "    #Implement this in practical\n",
        "    \"\"\"Model that fuses CNN and MLP representations.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(FusedModel1, self).__init__()\n",
        "        self.cnn_encoder = CNNEncoder()\n",
        "        self.mlp_encoder = MLP()\n",
        "        self.fc = nn.Linear(64*7*7 + 128, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        cnn_out = self.cnn_encoder.inner_features(x)\n",
        "        mlp_out = self.mlp_encoder.inner_features(x)\n",
        "        # average the two representations\n",
        "        x = torch.cat((cnn_out, mlp_out), dim=1)\n",
        "        fused = self.fc(x)\n",
        "        return fused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FusedModel2(nn.Module):\n",
        "    #Implement this in practical\n",
        "    \"\"\"Model that fuses CNN and MLP representations.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(FusedModel2, self).__init__()\n",
        "        self.cnn_encoder = CNNEncoder()\n",
        "        self.mlp_encoder = MLP()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        cnn_out = self.cnn_encoder(x)\n",
        "        mlp_out = self.mlp_encoder(x)\n",
        "        # select the maximum of the two representations\n",
        "        fused = torch.max(cnn_out, mlp_out)\n",
        "        return fused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FusedModel1(\n",
              "  (cnn_encoder): CNNEncoder(\n",
              "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (fc1): Linear(in_features=3136, out_features=10, bias=True)\n",
              "  )\n",
              "  (mlp_encoder): MLP(\n",
              "    (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=3264, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fused_nn = FusedModel1()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "fuse_optimizer =  optim.Adam(fused_nn.parameters(), lr=learning_rate)\n",
        "fused_nn.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Fusion Encoder train loss 0.0045247434695206935\n",
            "Epoch 1: Fusion Encoder val loss 0.23469054387178687, accuracy 0.9293\n",
            "Epoch 2: Fusion Encoder train loss 0.0024358755221723445\n",
            "Epoch 2: Fusion Encoder val loss 0.1373283449014028, accuracy 0.9592\n",
            "Epoch 3: Fusion Encoder train loss 0.002255846948895071\n",
            "Epoch 3: Fusion Encoder val loss 0.13401362885534762, accuracy 0.9608\n",
            "Epoch 4: Fusion Encoder train loss 0.001996168175800925\n",
            "Epoch 4: Fusion Encoder val loss 0.14508918918503655, accuracy 0.9559\n",
            "Epoch 5: Fusion Encoder train loss 0.0019409410678947877\n",
            "Epoch 5: Fusion Encoder val loss 0.14856585454609658, accuracy 0.9584\n",
            "Done training! Evaluating on test set...\n",
            "Fusion Encoder test loss 0.1254173233255744, accuracy 0.9630\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the Fused encoder\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = train_model(fused_nn, train_loader, fuse_optimizer, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: Fusion Encoder train loss {avg_loss}\")\n",
        "    avg_loss, accuracy = evaluate_model(fused_nn, val_loader, criterion, device=device)\n",
        "    print(f\"Epoch {epoch+1}: Fusion Encoder val loss {avg_loss}, accuracy {accuracy:.4f}\")\n",
        "\n",
        "print(\"Done training! Evaluating on test set...\")\n",
        "#Test\n",
        "avg_loss, accuracy = evaluate_model(fused_nn, test_loader, criterion, device=device)\n",
        "print(f\"Fusion Encoder test loss {avg_loss}, accuracy {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Exlo5W6s8Zl"
      },
      "source": [
        "## (v) Investigate how adding noise to the representations impacts performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQvEExABKmtT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9xjVx0utU4K"
      },
      "source": [
        "## (vi) Self-supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsGQS_61tZek"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh12pnmItZzI"
      },
      "source": [
        "## (vii) Alignment noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxJ0-c5TtZ_C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
